FROM llama2:latest

# sets the temperature to 1 [higher is more creative, lower is more coherent]

PARAMETER temperature 0.5

# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token

PARAMETER num_ctx 4096
PARAMETER num_gpu 50

# sets a custom system prompt to specify the behavior of the chat assistant

SYSTEM
